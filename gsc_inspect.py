import os, json, time, random, threadingfrom pathlib import Pathimport pandas as pdimport requestsfrom concurrent.futures import ThreadPoolExecutor, as_completedfrom google.oauth2.credentials import Credentialsfrom google_auth_oauthlib.flow import InstalledAppFlowfrom google.auth.transport.requests import Request as GARequestfrom googleapiclient.discovery import build# ---- Rich UI ----from rich.console import Consolefrom rich.panel import Panelfrom rich.table import Tablefrom rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumnfrom rich.text import Textfrom rich.columns import Columnsconsole = Console()# ========= CONFIG =========URLS_XLSX = Path('urls.xlsx')     # <— relative path (safer)CLIENT_SECRET = "client_secret.json"TOKEN_JSON = 'token.json'GSC_SITE_URL = 'https://www.figma.com/'   # must match a verified property in GSCEXPORT_XLSX = 'export.xlsx'SCOPES = ['https://www.googleapis.com/auth/webmasters.readonly']REQUEST_TIMEOUT_S = 60WORKERS = 12                       # start moderate; raise if you don't see 429sMAX_RETRIES = 3VERBOSE_URL_LOGS = False# ==========================# ---------- thread-safe logging ----------_PRINT_LOCK = threading.Lock()def log(msg):    with _PRINT_LOCK:        console.log(msg)# ---------- auth ----------def get_credentials():    creds = None    if os.path.exists(TOKEN_JSON):        creds = Credentials.from_authorized_user_file(TOKEN_JSON, SCOPES)    if not creds or not creds.valid:        if creds and creds.expired and creds.refresh_token:            creds.refresh(GARequest())        else:            flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET, SCOPES)            creds = flow.run_local_server(port=0)        with open(TOKEN_JSON, 'w') as f:            f.write(creds.to_json())    return credsdef get_access_token(creds: Credentials) -> str:    if not creds.valid:        creds.refresh(GARequest())        with open(TOKEN_JSON, 'w') as f:            f.write(creds.to_json())    return creds.tokendef list_verified_sites(service):    site_list = service.sites().list().execute()    return [        s['siteUrl'] for s in site_list.get('siteEntry', [])        if s.get('permissionLevel') != 'siteUnverifiedUser'        and s.get('siteUrl', '').startswith('http')  # ignores sc-domain: entries    ]# ---------- extraction ----------def extract_index_result_full(resp_json: dict) -> dict:    out = {}    ir = (resp_json or {}).get("inspectionResult", {})    idx = ir.get("indexStatusResult", {})    mu  = ir.get("mobileUsabilityResult", {})    out["inspectionResultLink"] = ir.get("inspectionResultLink", "No Data")    out["verdict"]          = idx.get("verdict", "No Data")    out["coverageState"]    = idx.get("coverageState", "No Data")    out["robotsTxtState"]   = idx.get("robotsTxtState", "No Data")    out["indexingState"]    = idx.get("indexingState", "No Data")    out["lastCrawlTime"]    = idx.get("lastCrawlTime", "No Data")    out["pageFetchState"]   = idx.get("pageFetchState", "No Data")    out["googleCanonical"]  = idx.get("googleCanonical", "No Data")    out["userCanonical"]    = idx.get("userCanonical", "No Data")    out["crawledAs"]        = idx.get("crawledAs", "No Data")    sitemaps = idx.get("sitemap") or []    out["sitemaps"] = "; ".join(sitemaps) if isinstance(sitemaps, list) else (sitemaps or "No Data")    refs = idx.get("referringUrls") or []    out["referringUrls"] = "; ".join(refs) if isinstance(refs, list) else (refs or "No Data")    out["mobileUsabilityVerdict"] = mu.get("verdict", "No Data")    return out# ---------- parallel plumbing ----------_thread_local = threading.local()_token_lock = threading.Lock()def _session():    if not hasattr(_thread_local, "session"):        _thread_local.session = requests.Session()    return _thread_local.sessiondef inspect_once(session, url, site_url, token):    endpoint = 'https://searchconsole.googleapis.com/v1/urlInspection/index:inspect'    headers = {'Authorization': f'Bearer {token}', 'Accept': 'application/json'}    payload = {"inspectionUrl": url, "languageCode": "en", "siteUrl": site_url}    resp = session.post(endpoint, headers=headers, json=payload, timeout=REQUEST_TIMEOUT_S)    try:        data = resp.json()    except Exception:        data = {"error": {"code": resp.status_code, "message": "Non-JSON response"}}    return resp.status_code, datadef worker(u, site_url, creds, token_holder):    if VERBOSE_URL_LOGS:        log(f"→ Inspecting: {u}")    session = _session()    backoff = 1.5    last_data = None    status = None    for attempt in range(1, MAX_RETRIES + 1):        status, data = inspect_once(session, u, site_url, token_holder['token'])        last_data = data        err = (data.get('error') or {})        err_code = err.get('code')        err_msg  = (err.get('message') or '').lower()        retryable = (            status in (401, 429) or            (500 <= status < 600) or            (status == 403 and ('rate' in err_msg or 'quota' in err_msg))        )        if status == 401 or err_code == 401:            with _token_lock:                token_holder['token'] = get_access_token(creds)        if not retryable:            break        if attempt < MAX_RETRIES:            time.sleep((backoff ** (attempt-1)) + random.uniform(0, 0.4))    row = extract_index_result_full(last_data if isinstance(last_data, dict) else {})    row['URL'] = u    row['_status_code'] = status    row['_error'] = (last_data.get('error') or {}).get('message') if isinstance(last_data, dict) else None    if VERBOSE_URL_LOGS:        log(f"✓ Done: {u} [status {status if status is not None else '?'}]")    return row# ---------- main ----------def main():    console.rule("[bold magenta]Google Search Console Index Checker")    # 1) OAuth + token    creds = get_credentials()    token_holder = {'token': get_access_token(creds)}    # 2) Build service (for listing/verification)    service = build('searchconsole', 'v1', credentials=creds)    verified_sites = list_verified_sites(service)    sites_list = "\n".join(f"[green]•[/] {s}" for s in verified_sites) or "[yellow]No verified URL-prefix properties found[/]"    site_ok = (GSC_SITE_URL in verified_sites)    console.print(        Panel.fit(            Text.from_markup(                f"[bold]Authenticated successfully with Google Search Console.[/]\n\n"                f"[bold]Verified GSC properties:[/]\n{sites_list}\n\n"                + (f"[green]Site is verified:[/] {GSC_SITE_URL}" if site_ok                   else f"[yellow]Warning:[/] {GSC_SITE_URL} is not in your verified URL-prefix properties.")            ),            title="🔐 Auth & Properties",            border_style="cyan"        )    )    # 3) Load URLs    if not URLS_XLSX.exists():        console.print(Panel(f"Could not find {URLS_XLSX.resolve()}", title="Input Error", border_style="red"))        raise SystemExit(1)    urls_df = pd.read_excel(URLS_XLSX)    if 'URL' not in urls_df.columns:        raise ValueError("Input Excel must contain a column named 'URL'")    urls = [u for u in urls_df['URL'].dropna().astype(str)]    total = len(urls)    console.print(Panel.fit(        f"Fetched [bold]{total}[/] URLs from [italic]{URLS_XLSX}[/].",        title="📥 Input",        border_style="white"    ))    # 4) Live progress + live table    table = Table(expand=True, show_lines=False)    table.add_column("URL", overflow="fold")    table.add_column("Status", justify="center")    table.add_column("CoverageStatus", justify="left")    progress = Progress(        TextColumn("[bold]Inspecting URLs…[/]"),        BarColumn(bar_width=None),        TextColumn("{task.percentage:>3.0f}%"),        TimeElapsedColumn(),        expand=True    )    ok = 0    failed = 0    results = []    with progress:        task = progress.add_task("inspect", total=total)        # draw an empty table once        console.print(table)        # run workers        with ThreadPoolExecutor(max_workers=WORKERS) as ex:            futures = {ex.submit(worker, u, GSC_SITE_URL, creds, token_holder): u for u in urls}            for fut in as_completed(futures):                row = fut.result()                results.append(row)                # Decide "Status" for the table                status_txt = "Success" if not row.get('_error') else "Failed"                cov = row.get('coverageState', 'No Data')                # update counts                if status_txt == "Success":                    ok += 1                else:                    failed += 1                # add a visible row immediately                table.add_row(row.get('URL', ''),                               f"[green]{status_txt}[/]" if status_txt == "Success" else f"[red]{status_txt}[/]",                              cov)                # clear previous table render and reprint                console.clear()                console.print(progress)   # redraw the progress bar at the top                console.print(table)      # then print the table below                # tick progress                progress.advance(task, 1)    # 5) Export    out_cols = [        'URL',        'inspectionResultLink',        'verdict',        'coverageState',        'robotsTxtState',        'indexingState',        'lastCrawlTime',        'pageFetchState',        'crawledAs',        'userCanonical',        'googleCanonical',        'sitemaps',        'referringUrls',        'mobileUsabilityVerdict',        '_status_code',        '_error'    ]    df = pd.DataFrame.from_records(results)    df = df[[c for c in out_cols if c in df.columns]]    df.to_excel(EXPORT_XLSX, index=False)    # 6) Summary panel    summary = Table.grid(padding=(0,1))    summary.add_row("Total URLs Processed:", f"[bold]{len(df)}[/]")    summary.add_row("Successful:", f"[bold green]{ok}[/]")    summary.add_row("Failed:", f"[bold red]{failed}[/]")    console.print(        Panel(            Columns([summary]),            title="🧾 Inspection Summary",            border_style="magenta",        )    )    console.print(f"[dim]Exported to {Path(EXPORT_XLSX).resolve()}[/]")if __name__ == "__main__":    main()