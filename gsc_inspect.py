import os, json, time, random, threading, sysimport pandas as pdimport requestsfrom concurrent.futures import ThreadPoolExecutor, as_completedfrom google.oauth2.credentials import Credentialsfrom google_auth_oauthlib.flow import InstalledAppFlowfrom google.auth.transport.requests import Request as GARequestfrom googleapiclient.discovery import build# ========= CONFIG =========URLS_XLSX = '/urls.xlsx'  # sheet with a column named "URL"CLIENT_SECRET = "client_secret.json"TOKEN_JSON = 'token.json'  # will be created & refreshed automaticallyGSC_SITE_URL = 'https://www.figma.com/'  # must match a verified property in your GSCEXPORT_XLSX = 'export.xlsx'SCOPES = ['https://www.googleapis.com/auth/webmasters.readonly']REQUEST_TIMEOUT_S = 60WORKERS = 20              # concurrency; tune 10–30; lower if you see many 429sMAX_RETRIES = 3PRINT_EVERY = 10          # progress heartbeat every N completed URLsVERBOSE_URL_LOGS = True   # set False if the per-URL logs are too chatty# ==========================# ---------- thread-safe logging ----------_PRINT_LOCK = threading.Lock()def log(msg):    with _PRINT_LOCK:        print(msg, flush=True)# ---------- auth ----------def get_credentials():    creds = None    if os.path.exists(TOKEN_JSON):        creds = Credentials.from_authorized_user_file(TOKEN_JSON, SCOPES)    if not creds or not creds.valid:        if creds and creds.expired and creds.refresh_token:            creds.refresh(GARequest())        else:            flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET, SCOPES)            creds = flow.run_local_server(port=0)        with open(TOKEN_JSON, 'w') as f:            f.write(creds.to_json())    return credsdef get_access_token(creds: Credentials) -> str:    if not creds.valid:        creds.refresh(GARequest())        with open(TOKEN_JSON, 'w') as f:            f.write(creds.to_json())    return creds.tokendef list_verified_sites(service):    site_list = service.sites().list().execute()    return [        s['siteUrl'] for s in site_list.get('siteEntry', [])        if s.get('permissionLevel') != 'siteUnverifiedUser'        and s.get('siteUrl', '').startswith('http')    ]# ---------- extraction ----------def extract_index_result_full(resp_json: dict) -> dict:    out = {}    ir = (resp_json or {}).get("inspectionResult", {})    idx = ir.get("indexStatusResult", {})    mu  = ir.get("mobileUsabilityResult", {})    out["inspectionResultLink"] = ir.get("inspectionResultLink", "No Data")    out["verdict"]          = idx.get("verdict", "No Data")    out["coverageState"]    = idx.get("coverageState", "No Data")    out["robotsTxtState"]   = idx.get("robotsTxtState", "No Data")    out["indexingState"]    = idx.get("indexingState", "No Data")    out["lastCrawlTime"]    = idx.get("lastCrawlTime", "No Data")    out["pageFetchState"]   = idx.get("pageFetchState", "No Data")    out["googleCanonical"]  = idx.get("googleCanonical", "No Data")    out["userCanonical"]    = idx.get("userCanonical", "No Data")    out["crawledAs"]        = idx.get("crawledAs", "No Data")    sitemaps = idx.get("sitemap") or []    out["sitemaps"] = "; ".join(sitemaps) if isinstance(sitemaps, list) else (sitemaps or "No Data")    refs = idx.get("referringUrls") or []    out["referringUrls"] = "; ".join(refs) if isinstance(refs, list) else (refs or "No Data")    out["mobileUsabilityVerdict"] = mu.get("verdict", "No Data")    return out# ---------- parallel plumbing ----------_thread_local = threading.local()_token_lock = threading.Lock()def _session():    if not hasattr(_thread_local, "session"):        _thread_local.session = requests.Session()    return _thread_local.sessiondef inspect_once(session, url, site_url, token):    endpoint = 'https://searchconsole.googleapis.com/v1/urlInspection/index:inspect'    headers = {'Authorization': f'Bearer {token}'}    payload = {"inspectionUrl": url, "languageCode": "en", "siteUrl": site_url}    resp = session.post(endpoint, headers=headers, json=payload, timeout=REQUEST_TIMEOUT_S)    try:        data = resp.json()    except Exception:        data = {"error": {"code": resp.status_code, "message": "Non-JSON response"}}    return resp.status_code, datadef worker(u, site_url, creds, token_holder):    if VERBOSE_URL_LOGS:        log(f"→ Inspecting: {u}")    session = _session()    backoff = 1.5    last_data = None    status = None    for attempt in range(1, MAX_RETRIES + 1):        status, data = inspect_once(session, u, site_url, token_holder['token'])        last_data = data        err_code = (data.get('error') or {}).get('code')        # Success or non-retryable        if status not in (0, 401, 429) and not (500 <= status < 600):            break        # 401 -> refresh token once (globally) then retry        if status == 401 or err_code == 401:            with _token_lock:                token_holder['token'] = get_access_token(creds)        if attempt < MAX_RETRIES:            time.sleep((backoff ** (attempt-1)) + random.uniform(0, 0.4))    row = extract_index_result_full(last_data if isinstance(last_data, dict) else {})    row['URL'] = u    row['_status_code'] = status    row['_error'] = (last_data.get('error') or {}).get('message') if isinstance(last_data, dict) else None    if VERBOSE_URL_LOGS:        status_txt = f"{status}" if status is not None else "?"        log(f"✓ Done: {u} [status {status_txt}]")    return row# ---------- main ----------# 1) OAuth + tokencreds = get_credentials()token_holder = {'token': get_access_token(creds)}# 2) Build service (for listing/verification)service = build('searchconsole', 'v1', credentials=creds)verified_sites = list_verified_sites(service)log("Verified GSC properties:")for s in verified_sites:    log(f" - {s}")if GSC_SITE_URL not in verified_sites:    log(f"\nWARNING: {GSC_SITE_URL} is not in your verified properties above.\n")else:    log("Looking good, Site is a verified property")# 3) Load URLsurls_df = pd.read_excel(URLS_XLSX)if 'URL' not in urls_df.columns:    raise ValueError("Input Excel must contain a column named 'URL'")urls = [u for u in urls_df['URL'].dropna().astype(str)]total = len(urls)log(f"Starting parallel inspection of {total} URLs with {WORKERS} workers…\n")# 4) Run in parallel with progress heartbeatsresults = []completed = 0with ThreadPoolExecutor(max_workers=WORKERS) as ex:    futures = {ex.submit(worker, u, GSC_SITE_URL, creds, token_holder): u for u in urls}    for fut in as_completed(futures):        results.append(fut.result())        completed += 1        if completed % PRINT_EVERY == 0 or completed == total:            log(f"Progress: {completed}/{total} completed")# 5) Exportout_cols = [    'URL',    'inspectionResultLink',    'verdict',    'coverageState',    'robotsTxtState',    'indexingState',    'lastCrawlTime',    'pageFetchState',    'crawledAs',    'userCanonical',    'googleCanonical',    'sitemaps',    'referringUrls',    'mobileUsabilityVerdict',    '_status_code',    '_error']df = pd.DataFrame.from_records(results)df = df[[c for c in out_cols if c in df.columns]]df.to_excel(EXPORT_XLSX, index=False)log(f"\nExported {len(df)} rows to {os.path.abspath(EXPORT_XLSX)}")